# =============================================================================
# Rust LLM Runner Configuration - TOP PERFORMANCE PRESET
# =============================================================================
# Copy this file to .env and modify as needed.
# These settings are optimized for MAXIMUM PERFORMANCE.

# =============================================================================
# Directory Settings
# =============================================================================
# RUST_LLM_MODELS_DIR=/path/to/models
# RUST_LLM_CACHE_DIR=/path/to/cache
# RUST_LLM_DB_PATH=/path/to/db

# =============================================================================
# Server Settings
# =============================================================================
RUST_LLM_HOST=127.0.0.1
RUST_LLM_PORT=11434

# =============================================================================
# ðŸš€ GPU SETTINGS (CRITICAL FOR PERFORMANCE)
# =============================================================================
# Offload ALL layers to GPU (999 = all layers)
RUST_LLM_GPU_LAYERS=999

# =============================================================================
# ðŸš€ PERFORMANCE TUNING
# =============================================================================
# Batch size - larger = faster throughput (512-2048 recommended for GPU)
RUST_LLM_BATCH_SIZE=512

# Number of CPU threads (auto-detected if not set)
# For GPU: set to half your CPU cores
# For CPU-only: set to all physical cores
# RUST_LLM_THREADS=8

# Context size - larger contexts use more VRAM
# 4096 = good balance, 8192+ for long conversations
RUST_LLM_CONTEXT_SIZE=4096

# Flash Attention is enabled by default for faster inference

# =============================================================================
# Model Settings
# =============================================================================
RUST_LLM_MAX_LOADED_MODELS=1

# =============================================================================
# Generation Settings
# =============================================================================
RUST_LLM_MAX_TOKENS=2048
RUST_LLM_TEMPERATURE=0.7
RUST_LLM_TOP_P=0.9
RUST_LLM_TOP_K=40
RUST_LLM_REPEAT_PENALTY=1.1

# =============================================================================
# Stream Mode (true/false)
# =============================================================================
# When enabled, tokens are printed as they are generated (real-time output)
# When disabled, waits for full response before printing
RUST_LLM_STREAM=true

# =============================================================================
# Logging
# =============================================================================
RUST_LLM_LOG_LEVEL=info

# =============================================================================
# ðŸš€ PERFORMANCE TIPS
# =============================================================================
# 1. Use quantized models (Q4_K_M, Q5_K_M) for best speed/quality balance
# 2. Ensure CUDA/Metal is enabled at compile time
# 3. For NVIDIA: Build with --features cuda
# 4. For Mac: Build with --features metal  
# 5. Reduce CONTEXT_SIZE if running out of VRAM
# 6. Increase BATCH_SIZE for higher throughput (if VRAM allows)
